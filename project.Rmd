---
title: "Practical Machine Learning - Course Project"
author: "Trieu Tran"
date: "November 14, 2015"
output: html_document
---
### Summary

This is the course project for the Practical Machine Learning class. The goal of this project is to build a prediction model for "how well" people do a simple "Unilateral Dumbbell Biceps Curl" exercise.  The data for buidling the model was collected from sensors attached to the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The quality of these participants' exercise was recorded and was categorized as Class A, Class B, Class C, Class D and Class E.  Class A corresponds to correct execution of the "Biceps Curl" exercise, the other 4 classes corresponds to incorrect executions with common mistakes. 

The report describes how we perform data cleaning-up; how we select different machine learning algorithms such as RPart, GBM and Random Forest on the training dataset; and how we determine the best prediction model and expected out-of-sample error.  To validate our prediction model, we run our model against the test data of 20 cases; and successfully get 20 correct predictions.

<strong>Data Source</strong>

The training data for this project are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

We would like to thank a group of researchers who let us use their dataset for this project.
<em>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises.](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.</em>

More information is available from the website here: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset). 

###Data Exploration

First, we download csv files from the above mentioned links, and then use R's function to parse these csv files to R datasets.  For data fields which contain NA, #DIV/0 or blank spaces, we treat them as NA.

```{r echo=TRUE, cache=TRUE, message=FALSE}
##http://icetornado.github.io/practical_machine_learning_project/
## loading libraries
library(caret)
library(randomForest)
library(doParallel)

registerDoParallel()

if (!file.exists("data")){
    dir.create("data")
}

# figureDir <- 'figure'
# if (!file.exists(figureDir)){
#     dir.create(figureDir)
# } 

## downloading csv files
if (!file.exists(file.path("data", "pml-training.csv"))) {
        message("Downloading training file")
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = file.path("data", "pml-training.csv"))
}

if (!file.exists(file.path("data", "pml-testing.csv"))) {
        message("Downloading testing file")
        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = file.path("data", "pml-testing.csv"))
}

##loading training and testing data from csv files
message("reading raw csv files")
rawTraining <- read.csv(file.path("data", "pml-training.csv"), stringsAsFactors=FALSE, na.strings = c("NA","#DIV/0!",""))
rawTesting <- read.csv(file.path("data", "pml-testing.csv"), stringsAsFactors=FALSE, na.strings = c("NA","#DIV/0!",""))
```

The <strong>caret</strong> package takes major part of creating predictive models for this project.  We first use the package's <strong>createDataPartition</strong> function to split (with split ratio 60/40) data from reading the training csv file into two datasets which are named as <strong>training</strong> and <strong>testing</strong>.

```{r echo=TRUE, cache=TRUE, message=FALSE}
set.seed(124567)
inTrain <- createDataPartition(y=rawTraining$classe, p=0.6, list=FALSE)
training <- rawTraining[inTrain, ]
testing <- rawTraining[-inTrain, ]

str(training, list.len=30)
table(training$classe)
```

### Data Cleaning
We inspect our training data set and find out there is quite a number of variables which store mostly NAs.  These variables don't contribute anything for our model building.  Therefore, we go through the list of variables, and remove ones which have NAs made up more than 80% of its data. In addition, we also remove the first 7 variables because they are not actual movement data from the sensors. 

A very handy function of the <strong>caret</strong> package, <strong>findCorrelation</strong>, helps us to find highly corelated variables.  We use the function with a cutoff setting at 0.5; and get a list a variables which we can remove from the data.  However, for testing whether "the more variables is the better", we keep the original 53-variable training and test dataset (named <strong>training</strong> and <strong>testing</strong>). We also create two new 22-variable datasets with exclusion of highly related variables: <strong>training1</strong> and <strong>testing1</strong>.

```{r echo=TRUE, cache=TRUE, message=FALSE}
## counting NAs in each column, and returning a list of columns which have  > 80% NAs
naList <- data.frame(ord = integer(0), name = character(0), cnNA = integer(0))
for(j in 1:length(training)) {
        if(sum(is.na(training[, j])) / length(training[,j]) > 0.8) {
                naList <- rbind(naList, data.frame(ord = j, name = names(training)[j], cnNA = sum(is.na(training[, j]))))
        }
}

## remove columns from the 1st to 7th ()
removeCols <- c(1:7, naList$ord)

training <- training[-removeCols]
testing <- testing[-removeCols]

## eliminating highly correlated variables
correlationMatrix <- cor(training[-53])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)

training1 = training[-highlyCorrelated]
testing1 = testing[-highlyCorrelated]
```

### Models Training

We consequentially train our models with <strong>Decision Tree</strong>, <strong>Adaptive Boosting</strong> and <strong>Random Forest</strong> algorithm on both <strong>training</strong> and <strong>training1</strong> dataset.  Each iteration of model training is set to be repeated 3 times and to be 10-fold cross-validation.  

#### Decision Tree with 22 variables
```{r cache=TRUE, echo=TRUE, message=FALSE}
## declaring fit control
fitControl <- trainControl(
        method = "repeatedcv",
        number = 10,
        repeats = 3)

registerDoParallel()
fitRPart <- train(classe ~ ., method = "rpart", data = training1, trControl = fitControl)
predictRPart <- predict(fitRPart, testing[, -22])
confusionRPart <- confusionMatrix(testing1[, 22], predictRPart)
print(confusionRPart)
```

#### Decision Tree with 53 variables
```{r cache=TRUE, echo=TRUE, message=FALSE}
registerDoParallel()
fitRPartBig <- train(classe ~ ., method = "rpart", data = training, trControl = fitControl)
predictRPartBig <- predict(fitRPartBig, testing[, -53])
confusionRPartBig <- confusionMatrix(testing[, 53], predictRPartBig)
print(confusionRPartBig)
```

#### GBM with 22 variables
```{r cache=TRUE, echo=TRUE, message=FALSE}
registerDoParallel()
fitGBM <- train(classe ~ ., method = "gbm", data = training1, verbose = FALSE, trControl = fitControl)
predictGBM <- predict(fitGBM, testing1[, -22])
confusionGBM <- confusionMatrix(testing1[, 22], predictGBM)
print(confusionGBM)
```

#### GBM with 53 variables
```{r cache=TRUE, echo=TRUE, message=FALSE}
registerDoParallel()
fitGBMBig <- train(classe ~ ., method = "gbm", data = training, verbose = FALSE, trControl = fitControl)
predictGBMBig <- predict(fitGBMBig, testing[, -53])
confusionGBMBig <- confusionMatrix(testing[, 53], predictGBMBig)
print(confusionGBMBig)
```

#### Random Forest with 22 variables
```{r cache=TRUE, echo=TRUE, message=FALSE}
registerDoParallel()
fitRF <- train(classe~ ., data=training1, method="rf", trControl=fitControl, verbose = FALSE)
predictRF <- predict(fitRF, newdata = testing1)
confusionRF <- confusionMatrix(predictRF, testing1$classe)
print(confusionRF)
```

#### Random Forest with 53 variables
```{r cache=TRUE, echo=TRUE, message=FALSE}
registerDoParallel()
fitMoreRF <- train(classe~ ., data=training, method="rf", trControl=fitControl, verbose = FALSE)
predictMoreRF <- predict(fitMoreRF, newdata = testing)
confusionMoreRF <- confusionMatrix(predictMoreRF, testing$classe)
print(confusionMoreRF)
```

#### Comparing models
```{r cache=TRUE, echo=TRUE, message=FALSE}
# comparing models
modelList <- list(
        RPart = fitRPart,
        RPart53 = fitRPartBig,
        GBM = fitGBM,
        GBM53 = fitGBMBig,
        RF = fitRF,
        RF53 = fitMoreRF)

resamps <- resamples(modelList)
dotplot(resamps, layout = c(3, 1))
```

The Random Forest algorithm yields the best results with accuracy rates are <strong>98.14%</strong> on the model built of 22 variables  (named as <strong>RF</strong>) and <strong>99.22%</strong> on the model built of 53 variables (named as <strong>RF53</strong>).  We decide to go with the model <strong>RF</strong> with 22 variables because of its simplicity and its acceptable accuracy.

<strong>Estimate of out-of-sample error</strong> is 100% - 98.14% = <strong>1.86%</strong>

### Project submission

We conduct testing on all models againts a dataset of 20 observation and print out the test results as follows:

```{r cache=TRUE, echo=TRUE, message=FALSE}
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=file.path("results",filename),quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

for(j in 1:length(modelList)) {
        tP <- predict(modelList[j],  newdata = rawTesting)
        print(tP)
        if(j == 5) {      ## the second best model
                pml_write_files(tP)
        }
}

```
From the above test outcomes, we can see that the model <strong>RF</strong> and the model <strong>RF53</strong> give identical results. As mentioned earlier we pick our second best model to generate text files to submit to the Coursera's server.  The feedback score from the server is a perfect 20/20.

